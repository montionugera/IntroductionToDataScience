{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Sample Project \n",
    "# Data Wrangling with MongoDB\n",
    "\n",
    "## Map Area: Charlotte, NC, United States\n",
    "\n",
    "https://www.openstreetmap.org/relation/177415\n",
    "\n",
    "http://metro.teczno.com/#charlotte\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1. Problems Encountered in the Map\n",
    "\n",
    "#### ANS : \n",
    "After initially downloading a small sample size of the Charlotte area and running it against a provisional check_data.py file, I noticed three main problems with the data, which I will discuss in the following order:\n",
    "\n",
    "##### Over-abbreviated street names (“West Sugar Creek Rd.”,\"East Jefferson Street Ste C\").\n",
    "\n",
    "To resolve this we need to map the abbreviated keywords with full one. (def process_address_street_name)\n",
    "\n",
    "##### Inconsistent postal codes (“NC28209”, “28105-4837”, “28226”).\n",
    "To resolve this we need to remove the unwanted prefix and unwanted suffix. (def process_address_post_code)\n",
    "\n",
    "##### “Incorrect” postal codes (Charlotte area zip codes all begin with “282” however a large portion of all documented zip codes were outside this region.)\n",
    "To resolve this we need ignore zipcode does not start with 282. (def should_ignore_addresss)\n",
    "\n",
    "#### The records that has be clean and ignore is:\n",
    "The records be cleaned street name is  8  records.\n",
    "\n",
    "The records be cleaned postcode is :  5  records.\n",
    "\n",
    "The records be ignore because postcode does not start with 282 :  186  records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "CREATED = [\"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "def shape_element(element, record_data={'clean_street': [], 'clean_postcode': [], 'ignore_postcode': []}):\n",
    "    # In particular the following things should be done:\n",
    "    # - you should process only 2 types of top level tags: \"node\" and \"way\"\n",
    "    def process_normal_attr(target_element, _node):\n",
    "        _node['type'] = target_element.tag\n",
    "        for k in target_element.attrib:\n",
    "            if k not in CREATED and k not in ['lat', 'lon']:\n",
    "                _node[k] = target_element.attrib[k]\n",
    "\n",
    "    # - all attributes of \"node\" and \"way\" should be turned into regular key/value pairs, except:\n",
    "    #     - attributes in the CREATED array should be added under a key \"created\"\n",
    "\n",
    "    def process_created(target_element, _node):\n",
    "        create_dict = {}\n",
    "        for create_key in CREATED:\n",
    "            if create_key in target_element.attrib:\n",
    "                create_dict[create_key] = target_element.attrib[create_key]\n",
    "        if len(create_dict) > 0:\n",
    "            _node['created'] = create_dict\n",
    "        return create_dict\n",
    "\n",
    "    #     - attributes for latitude and longitude should be added to a \"pos\" array,\n",
    "    #       for use in geospacial indexing. Make sure the values inside \"pos\" array are floats\n",
    "    #       and not strings.\n",
    "\n",
    "    def process_geo(target_element, _node):\n",
    "        pos = []\n",
    "        for pos_key in ['lat', 'lon']:\n",
    "            if pos_key in target_element.attrib:\n",
    "                pos.append(float(target_element.attrib[pos_key]))\n",
    "        if len(pos) == 2:\n",
    "            _node[\"pos\"] = pos\n",
    "        return pos\n",
    "\n",
    "    # - if second level tag \"k\" value contains problematic characters, it should be ignored\n",
    "    # - if there is a second \":\" that separates the type/direction of a street,\n",
    "    #   the tag should be ignored,\n",
    "\n",
    "    def should_ignore_tag(target_element):\n",
    "        return problemchars.match(target_element.attrib['k']) or \"street:\" in target_element.attrib['k']\n",
    "\n",
    "    def should_ignore_addresss(addresss):\n",
    "        postcode = addresss.get(\"postcode\", None)\n",
    "        return postcode is not None and not postcode.startswith(\"282\")\n",
    "\n",
    "    # - if second level tag \"k\" value starts with \"addr:\", it should be added to a dictionary \"address\"\n",
    "\n",
    "    def is_address_tag(target_element):\n",
    "        return target_element.attrib['k'].startswith(\"addr:\")\n",
    "\n",
    "    street_mapping = {\"S\": \"South\",\n",
    "                      \"Ste\": \"Suite\",\n",
    "                      \"St.\": \"Street\",\n",
    "                      \"St\": \"Street\",\n",
    "                      \"Rd\": \"Road\",\n",
    "                      \"Rd.\": \"Road\"\n",
    "                      }\n",
    "\n",
    "    def process_address_street_name(street_name):\n",
    "\n",
    "        for abbv in [\"Ste\", \"St.\", \"St\", \"Rd\", \"Rd.\", \"S\"]:\n",
    "            if abbv + \" \" in street_name or street_name.endswith(abbv):\n",
    "                record_data['clean_street'].append(street_name)\n",
    "                street_name = street_name.replace(abbv, street_mapping[abbv])\n",
    "        return street_name\n",
    "\n",
    "    def process_address_post_code(post_code):\n",
    "\n",
    "        if re.compile(r'^[a-zA-Z]{2}[0-9]{5}$', re.IGNORECASE).search(post_code):\n",
    "            record_data['clean_postcode'].append(post_code)\n",
    "            return post_code[2:]\n",
    "        if len(post_code) > 5:\n",
    "            record_data['clean_postcode'].append(post_code)\n",
    "        return post_code[:5]\n",
    "\n",
    "    def process_address_tag(target_element, address={}):\n",
    "        k = target_element.attrib['k'].replace(\"addr:\", \"\")\n",
    "        if k == 'street':\n",
    "            address[k] = process_address_street_name(target_element.attrib['v'])\n",
    "        elif k == 'postcode':\n",
    "            address[k] = process_address_post_code(target_element.attrib['v'])\n",
    "        else:\n",
    "            address[k] = target_element.attrib['v']\n",
    "        return address\n",
    "\n",
    "    # - if second level tag \"k\" value does not start with \"addr:\", but contains \":\", you can process it\n",
    "    #   same as any other tag.\n",
    "\n",
    "    def process_way_sub_element(way_element, _node={}):\n",
    "        node_refs = []\n",
    "        for nd in way_element.iter(\"nd\"):\n",
    "            node_refs.append(nd.attrib['ref'])\n",
    "        _node[\"node_refs\"] = node_refs\n",
    "\n",
    "    node = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\":\n",
    "        process_normal_attr(element, node)\n",
    "        process_created(element, node)\n",
    "        process_geo(element, node)\n",
    "\n",
    "        address = {}\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            if not should_ignore_tag(tag):\n",
    "                if is_address_tag(tag):\n",
    "                    process_address_tag(tag, address=address)\n",
    "                    if should_ignore_addresss(address):\n",
    "                        record_data['ignore_postcode'].append(address)\n",
    "                        return None\n",
    "                else:\n",
    "                    node[tag.attrib['k']] = tag.attrib['v']\n",
    "        if len(address) > 0:\n",
    "            node['address'] = address\n",
    "        if element.tag == \"way\":\n",
    "            process_way_sub_element(element, node)\n",
    "        return node\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The records be cleaned street name is :  8  records.\n",
      "The records be cleaned postcode is :  5  records.\n",
      "The records be ignore because postcode does not start with 282 :  186  records.\n"
     ]
    }
   ],
   "source": [
    "def process_map(file_in, pretty=False):\n",
    "    import xml.etree.cElementTree as ET\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    record_data={'clean_street': [], 'clean_postcode': [], 'ignore_postcode': []}\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element,record_data)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2) + \"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    \n",
    "    print 'The records be cleaned street name is : ',str(len(record_data['clean_street'])) ,' records.'\n",
    "    print 'The records be cleaned postcode is : ',str(len(record_data['clean_postcode'])) ,' records.'\n",
    "    print 'The records be ignore because postcode does not start with 282 : ',str(len(record_data['ignore_postcode'])) ,' records.'\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client.examples\n",
    "    db.char.insert(data)\n",
    "    return data\n",
    "\n",
    "OSMFILE = 'charlotte.osm'\n",
    "data = process_map(OSMFILE, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort postcodes by count, descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': u'29732', u'count': 313}\n",
      "{u'_id': u'28097', u'count': 3}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client.examples\n",
    "pipeline = [\n",
    "            {\"$match\": {\"address.postcode\": {\"$exists\":1}}},\n",
    "            {\"$group\":{\"_id\":\"$address.postcode\",\"count\":{\"$sum\":1}}},\n",
    "            {\"$sort\": {\"count\":-1}} \n",
    "]\n",
    "result = [doc for doc in db.char.aggregate(pipeline)]\n",
    "import pprint\n",
    "pprint.pprint(result[0])\n",
    "pprint.pprint(result[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort cities by count, descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'Rock Hill', u'count': 337},\n",
      " {u'_id': u'Pineville', u'count': 81},\n",
      " {u'_id': u'Charlotte', u'count': 80},\n",
      " {u'_id': u'York', u'count': 72},\n",
      " {u'_id': u'Matthews', u'count': 30},\n",
      " {u'_id': u'Concord', u'count': 12},\n",
      " {u'_id': u'Lake Wylie', u'count': 6},\n",
      " {u'_id': u'Locust', u'count': 3},\n",
      " {u'_id': u'Monroe', u'count': 3},\n",
      " {u'_id': u'Fort Mill, SC', u'count': 3},\n",
      " {u'_id': u'Belmont, NC', u'count': 3},\n",
      " {u'_id': u'Rock Hill, SC', u'count': 3}]\n"
     ]
    }
   ],
   "source": [
    "pipeline =  [{\"$match\":{\"address.city\":{\"$exists\":1}}},\n",
    " {\"$group\":{\"_id\":\"$address.city\", \"count\":{\"$sum\":1}}}, \n",
    " {\"$sort\":{\"count\":-1}}]\n",
    "result = [doc for doc in db.char.aggregate(pipeline)]\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are the data not belong to Charlotte city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2. Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charlotte.osm : 294.21 MB\n",
      "charlotte.osm.json : 398.77 MB\n"
     ]
    }
   ],
   "source": [
    "suffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n",
    "def humansize(nbytes):\n",
    "    if nbytes == 0: return '0 B'\n",
    "    i = 0\n",
    "    while nbytes >= 1024 and i < len(suffixes)-1:\n",
    "        nbytes /= 1024.\n",
    "        i += 1\n",
    "    f = ('%.2f' % nbytes).rstrip('0').rstrip('.')\n",
    "    return '%s %s' % (f, suffixes[i])\n",
    "\n",
    "print 'charlotte.osm : '+humansize(os.path.getsize('charlotte.osm'))\n",
    "print 'charlotte.osm.json : '+humansize(os.path.getsize('charlotte.osm.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1571411"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.char.find().count()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1486064"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.char.find({\"type\":\"node\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85347"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.char.find({\"type\":\"way\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db.char.distinct(\"created.user\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 1 contributing user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'_id': u'jumbanho', u'count': 831567}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qry = db.char.aggregate([{\"$group\":{\"_id\":\"$created.user\", \"count\":{\"$sum\":1}}}, {\"$sort\":{\"count\":-1}}, {\"$limit\":1}])\n",
    "\n",
    "result = [doc for doc in qry]\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of users appearing only once (having 1 post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'_id': 1, u'num_users': 56}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qry = db.char.aggregate([{\"$group\":{\"_id\":\"$created.user\", \"count\":{\"$sum\":1}}}, \n",
    "                   {\"$group\":{\"_id\":\"$count\", \"num_users\":{\"$sum\":1}}}, \n",
    "                   {\"$sort\":{\"_id\":1}}, {\"$limit\":1}])\n",
    "\n",
    "result = [doc for doc in qry]\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of chosen type of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "university : 2\n",
      "arts_centre : 1\n",
      "marketplace : 1\n",
      "toilets : 7\n",
      "college : 1\n",
      "nightclub : 4\n",
      "pool : 1\n",
      "food_court : 1\n",
      "swimming_pool : 6\n",
      "drinking_water : 1\n",
      "community_centre : 1\n",
      "veterinary : 1\n",
      "closed : 1\n",
      "taxi : 2\n",
      "parking_entrance : 9\n",
      "bank : 16\n",
      "atm : 4\n",
      "pub : 3\n",
      "bicycle_parking : 2\n",
      "convenience : 3\n",
      "doctors : 1\n",
      "shelter : 15\n",
      "post_office : 12\n",
      "assisted_living : 1\n",
      "cinema : 7\n",
      "library : 33\n",
      "place_of_worship : 592\n",
      "bar : 4\n",
      "grave_yard : 82\n",
      "police : 7\n",
      "theatre : 7\n",
      "kindergarten : 2\n",
      "public_building : 2\n",
      "bus_station : 1\n",
      "telephone : 4\n",
      "fast_food : 72\n",
      "car_wash : 11\n",
      "dentist : 2\n",
      "fire_station : 52\n",
      "townhall : 8\n",
      "parking : 347\n",
      "restaurant : 124\n",
      "car_rental : 1\n",
      "prison : 2\n",
      "hospital : 22\n",
      "bench : 31\n",
      "post_box : 3\n",
      "pharmacy : 22\n",
      "waste_basket : 4\n",
      "fountain : 12\n",
      "cafe : 9\n",
      "fuel : 39\n",
      "courthouse : 1\n",
      "school : 422\n"
     ]
    }
   ],
   "source": [
    "#db.char.distinct(\"amenity\")\n",
    "qry = db.char.aggregate([{\"$match\":{\"amenity\":{\"$exists\":1}}},\n",
    "                        {\"$group\":{\"_id\":\"$amenity\", \"count\":{\"$sum\":1}}}, ])\n",
    "result = [doc for doc in qry]\n",
    "for node_info in result:\n",
    "    print \"%s : %s\"%(node_info['_id'],node_info['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3. Additional Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###ANS\n",
    "#####  To increase the number of data, we can motivate contributor to provide more data by give them a credit on website.\n",
    "##### To impute the missing values we can find the some data by other one attribute for example we can find the city name from postcode and if you use google api you can also find the postcode by lat,lng.\n",
    "##### To increase correctness of data we can cross-validate the data with other such as google api .\n",
    "\n",
    "ref : http://stackoverflow.com/questions/9689692/google-reverse-geocoding-how-to-snap-to-nearest-full-postcode\n",
    "ref : https://developers.google.com/maps/documentation/geocoding/intro\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 appearing amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'place_of_worship', u'count': 592},\n",
      " {u'_id': u'school', u'count': 422},\n",
      " {u'_id': u'parking', u'count': 347},\n",
      " {u'_id': u'restaurant', u'count': 124},\n",
      " {u'_id': u'grave_yard', u'count': 82},\n",
      " {u'_id': u'fast_food', u'count': 72},\n",
      " {u'_id': u'fire_station', u'count': 52},\n",
      " {u'_id': u'fuel', u'count': 39},\n",
      " {u'_id': u'library', u'count': 33},\n",
      " {u'_id': u'bench', u'count': 31}]\n"
     ]
    }
   ],
   "source": [
    "qry = db.char.aggregate([{\"$match\":{\"amenity\":{\"$exists\":1}}}, {\"$group\":{\"_id\":\"$amenity\",\n",
    "\"count\":{\"$sum\":1}}}, {\"$sort\":{\"count\":-1}}, {\"$limit\":10}])\n",
    "\n",
    "result = [doc for doc in qry]\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biggest religion (no surprise here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'christian', u'count': 582}]\n"
     ]
    }
   ],
   "source": [
    "qry = db.char.aggregate([{\"$match\":{\"amenity\":{\"$exists\":1}, \"amenity\":\"place_of_worship\"}},\n",
    "                                                \n",
    "{\"$group\":{\"_id\":\"$religion\", \"count\":{\"$sum\":1}}},\n",
    "                                                \n",
    "{\"$sort\":{\"count\":-1}}, {\"$limit\":1}])\n",
    "\n",
    "result = [doc for doc in qry]\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most popular cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': None, u'count': 65}, {u'_id': u'pizza', u'count': 10}]\n"
     ]
    }
   ],
   "source": [
    "qry = db.char.aggregate([{\"$match\":{\"amenity\":{\"$exists\":1}, \"amenity\":\"restaurant\"}}, \n",
    "                         {\"$group\":{\"_id\":\"$cuisine\", \"count\":{\"$sum\":1}}},        \n",
    "                         {\"$sort\":{\"count\":-1}}, {\"$limit\":2}])\n",
    "\n",
    "result = [doc for doc in qry]\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some of attributes are boolean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANS :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'yes', u'no', u'designated']\n"
     ]
    }
   ],
   "source": [
    "qry = db.char.distinct(\"bicycle\")\n",
    "\n",
    "result = [doc for doc in qry]\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
